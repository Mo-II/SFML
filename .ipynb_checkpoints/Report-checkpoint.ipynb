{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4123e5e7-6b0f-45e5-92cf-d51e84dfcdb8",
   "metadata": {},
   "source": [
    "# Report Statistical Foundations of Machine Learning\n",
    "\n",
    "- Khalil Oauld Chaib  0557031  Master of Applied Informatics: Artificial Intelligence\n",
    "- Mohammed Shabot     0563065  Master of Science in Applied Sciences and Engineering: Computer Science: Artificial Intelligence\n",
    "- Ferit Fikri Murad   0620940  Master of Applied Informatics: Artificial Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8a42fb-0e19-4690-98b6-1e807773cdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.4.2 pandas-2.2.2 pytz-2024.1 scikit-learn-1.5.0 threadpoolctl-3.5.0 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c72b8fd-132c-4b18-8dc7-69c875629452",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpre_processing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selkection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n",
      "File \u001b[0;32m~/Desktop/Mo/Statistical/SFML/Dataset/pre_processing.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from Dataset.pre_processing import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selkection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nn import datasplitter\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02802a8b-65b5-4b37-8067-ab3c61f1c81e",
   "metadata": {},
   "source": [
    "In this project for the course: 'Statistical Foundations of Machine Learning', we are expected to explore and address three fundamental research questions. We have to address these research questions in the form of an interactive report, combining textual explanations, visualizations and executable Python code to provide a comprehensive understanding of the research undertaken. These are the three research questions we chose:\n",
    "\n",
    "1. Which features have the highest impact on predicting obesity or CVD risk?\n",
    "2. What is the effect of hyperparameter choices, such as learning rate, batch size, or number of hidden units, on the performance of machine learning algorithms?\n",
    "3. What is the impact of feature imbalance and mislabeling on the performance of machine learning models?\n",
    "\n",
    "To address these research questions, we utilized two Machine learning models: Random forests and Neural Networks. Both these machine learning models will be explained in the next chapters. Thereafter, we will briefly discuss the pre processing phase. Lastly, we discuss the thorough experimentation and discuss the results in the last chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4e8c5-9dbb-420b-a30a-08aa78b922dd",
   "metadata": {},
   "source": [
    "## Research\n",
    "### Random forests\n",
    "Random forests is an ensemble learning method, which, as the name suggests, is based on constructing multiple decision trees. Random forests can be used for both classification and regression tasks. In the case of classification, the result of the random forest depends on the class that is returned by the majority of the trees. For regression, the result of the random forest is the mean of the outputs from all the trees. Decision trees are known to overfit, especially when noisy data is added, but random forests mitigate this problem by averaging the results of multiple trees, thus enhancing robustness and generalizability.\n",
    "\n",
    "As mentioned above, random forests is an ensemble learning method. The model uses __bagging__ (Bootstrap Aggregating) to increase the performance and robustness. Bagging is a method that combines multiple models to decrease variance and increase accuracy, where the model on itself might lack this. On a high level, the process of random forests consists out of 3 steps.\n",
    "\n",
    "1. __Bootstrap Samples__:\n",
    "   The dataset is split up into multiple subsets (bootstrap samples) by means of random sampling with replacement. This means that each         dataset has the same amount of samples, however, some observations might be selected multiple times, while other observations might not      be selected at all.\n",
    "2. __Model training__:\n",
    "   Each individual model will be trained on each bootstrap sample. In case of random forest, these models are decision trees. This results      in each model being trained on a slightly different dataset, thus resulting in slightly different decision trees.\n",
    "3. __Aggregating__:\n",
    "   For classification problems, the random forest determines the final result by selecting the class predicted by the majority of the trees     (majority voting). For regression problems, the final prediction is the average of the predictions from all the trees.\n",
    "\n",
    "Lets delve into some mathematics. \n",
    "Given a dataset D with N samples, bootstrap sampling will create B different subsets D<sub>1</sub>, D<sub>2</sub>, ..., D<sub>B</sub>, each of size N, by sampling from D with replacement. The probability of any given sample being included in a single bootstrap is equal to the following formula (Efron & Tibshirani, 1993):\n",
    "\n",
    "$$P(x_i \\in D_j) = 1 - \\left(1 - \\frac{1}{N}\\right)^N$$\n",
    "\n",
    "Each decision tree T<sub>i</sub> in the forest is trained on a bootstrap sample D<sub>i</sub>.At each point where the tree splits, only a random selection of  m  features out of the total  p  features is considered to find the best split. This added randomness makes the trees less similar to each other, which helps to make the overall model stronger and more reliable.\n",
    "\n",
    "Lastly, after all the individual trees have been trained on the bootstrap samples, we have to aggregate the predictions.\n",
    "\n",
    "__Classification__: When a new input is given to the random forest, each tree in the forest will make its own prediction T<sub>i</sub>(x).\n",
    "                    The final prediction is determined by a majority vote, which looks as follows: $$\\hat{y} = \\text{mode}(T_1(x), T_2(x), \\ldots, T_B(x))$$\n",
    "\n",
    "__Regression__: When a new input is given to the random forest, each tree provides its own prediction T<sub>i</sub>(x). The final prediction $$\\hat{y}$$ will be determined be the following formula: $$\\hat{y} = \\frac{1}{B} \\sum_{i=1}^{B} T_i(x)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1244138-b816-4491-9b79-277ff368e4f2",
   "metadata": {},
   "source": [
    "### Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e779ef-1463-4284-9231-2819531cf2dd",
   "metadata": {},
   "source": [
    "Neural networks are a class of machine learning models that are inspired by the human brain's structure and function, in that sense that they are designed to recognize complex structures and patterns in data through a series of interconnected nodes, called neurons, which are organized into layers. Their applications are enormous, they range from classification and regression, to image and speech recognition.\n",
    "\n",
    "### High level overview of neural networks\n",
    "A neural network typically consists of 3 types of layers, namely: The input layer, the hidden layer and the output layer. The training of a neural network can be summarized into the following steps (high level):\n",
    "\n",
    "1. The weights of the connections between the neurons are initialized; In our case, scikit-learn's MLPClassifier handles this initialization.\n",
    "2. Before training, you have to make sure that your non-numerical columns are encoded, and that the features are standardized. We achieved this by using LabelEncoder and StandardScaler.\n",
    "3. __Forward Propagation__: Data from the input layer is passed through the network, where each neuron will apply a transformation to the input. This continues through each layer, until the data reaches the third layer (output layer), and the predictions are made.\n",
    "4. __Loss Calculation__: After the predictions are made, they are compared to the actual target values using a loss function. The loss function measures the distance between the predicted values and the true values. The MLPClassifier, which is in our case, uses the cross-entropy loss function for classification tasks.\n",
    "5. __Backward Propagation__: Subsequently, the network will adjust its weight, so that the loss is minimzed. This is done through a backpropagation algorithm, which calculates the gradient of the loss function with respect to each weight and updates the weights accordingly using an optimization method like gradient descent.\n",
    "6. __Iteration__: Steps 2 - 4 are repeated for many iterations, which are called epochs, until the network's performance stabilizes and loss is minimzed.\n",
    "\n",
    "A key aspect to consider when training neural networks is the activation functions. Each neuron in the network receives input from all the neurons in the previous layer, and an activation function is applied to all these inputs. The activation function will determine whether the neuron is activated (passes a significant signal) or not (outputs zero). In our grid search algorithm (which will be mentioned later on in this notebook), we consider two activation functions:\n",
    "\n",
    "- __ReLU__ (Rectified Linear Unit):\n",
    "  The ReLU activation function is defined as follows: $$ \\text{ReLU}(z) = \\max(0, z) $$\n",
    "  where z is the weighted sum of the neurons from the previous layer, which is defined as follows: $$ z = \\sum_{i} w_i x_i + b $$\n",
    "  The ReLU activation function helps in mitigating the vanishing gradient problem, making it easier to train deep networks. It is   computationally efficient and helps the network to converge faster.\n",
    "- __Tanh__ (hyperbolic tangent):\n",
    "  The tanh activation function is defined as follows:\n",
    "  $$ \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} $$\n",
    "  where z, in the same fashion as above, is the weighted sum of the neurons from the previous layer, which is defined as follows:\n",
    "  $$ z = \\sum_{i} w_i x_i + b $$\n",
    "  The tanh activation function outputs values between -1 and 1, making it centered around zero, which can be useful during training. It is   often used in hidden layers to provide a smooth gradient and better convergence properties.\n",
    "\n",
    "Another key aspect to consider is gradient-based optimization. After backpropagation, where all the gradients are computed, the weights are then adjusted accordingly using a gradient-based optimization method. For our experiments, we chose to work with Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation.\n",
    "\n",
    "__SGD__: Stochastic Gradient Descent is, as mentioned above, an optimization method to minimize the loss function by updating the weights iteratively. The weights are updated as follows: $$ w_{t+1} = w_t - \\eta \\nabla L(w_t) $$\n",
    "\n",
    "where:\n",
    "- w<sub>t</sub> is the weight vector at iteration\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla$ L(w_t) is the gradient of the loss function L with respect to  w  at iteration t\n",
    "\n",
    "    \n",
    "__Adam__:   Adam is an optimization algorithm that combines the benefits of both the AdaGrad and RMSProp algorithms to minimize the loss function by updating the weights iteratively. The weights are updated using the following equations (Kingma & Ba, 2014):\n",
    "\n",
    "- $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(w_t)$\n",
    "- $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L(w_t))^2$\n",
    "- $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n",
    "- $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n",
    "- $w_{t+1} = w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
    "\n",
    "where:\n",
    "- $w_t$ is the weight vector at iteration $t$\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla L(w_t)$ is the gradient of the loss function $L$ with respect to $w$ at iteration $t$\n",
    "- $m_t$ and $v_t$ are the first and second moment estimates at iteration $t$\n",
    "- $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the moment estimates\n",
    "- $\\hat{m}_t$ and $\\hat{v}_t$ are the bias-corrected moment estimates\n",
    "- $\\epsilon$ is a small constant to prevent division by zero\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25e955-7532-4217-a5fa-d9ca728e8d92",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76280b32-c62c-4374-80bf-03d074917826",
   "metadata": {},
   "source": [
    "In this study, we utilized the dataset which is focused on obesity classification. It includes features concerning eating habits and physical condition.\n",
    "\n",
    "__Eating habits__:\n",
    "- FAVC: Frequenct consumtion of high caloric food\n",
    "- FCVC: Frequency of consumption of vegetables\n",
    "- NCP: Number of main meals\n",
    "- CAEC: Consumption of good between meals\n",
    "- CH20: Consumption of water daily\n",
    "- CALC: Consumption of alcohol\n",
    "\n",
    "__Physical condition__:\n",
    "\n",
    "- SCC: Calories consumption monitrong\n",
    "- FAF: Physical activity frequency\n",
    "- TUE: Time using technology devices\n",
    "- MTRANS: Transportation used\n",
    "\n",
    "Other variables that are obtained are: Gender, Age, Height and Weight. Lastly, there are 6 obesity types, which are the target values:\n",
    "•Underweight Less than 18.5\n",
    "•Normal 18.5 to 24.9\n",
    "•Overweight 25.0 to 29.9\n",
    "•Obesity I 30.0 to 34.9\n",
    "•Obesity II 35.0 to 39.9\n",
    "•Obesity III Higher than 40\n",
    "\n",
    "All this data, can be found on https://www.kaggle.com/datasets/aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2b32fd-ecf1-4034-a95f-e51f00f55bb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Dataset/Dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m dataset\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = pd.read('/Dataset/Dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060465e6-9ff7-43bd-a9e5-7cfa4d1933d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
